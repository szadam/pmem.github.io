<!doctype html><html dir=ltr lang=en-us><head><link rel=stylesheet type=text/css href=/css/style.css><meta property="og:title" content="librpma | PMDK"><meta property="og:description" content="librpma.7 rpma_atomic_write.3 rpma_conn_apply_remote_peer_cfg.3 rpma_conn_cfg_delete.3 rpma_conn_cfg_get_compl_channel.3 rpma_conn_cfg_get_cq_size.3 rpma_conn_cfg_get_rcq_size.3 rpma_conn_cfg_get_rq_size.3 rpma_conn_cfg_get_sq_size.3 rpma_conn_cfg_get_srq.3 rpma_conn_cfg_get_timeout.3 rpma_conn_cfg_new.3 rpma_conn_cfg_set_compl_channel.3 rpma_conn_cfg_set_cq_size.3 rpma_conn_cfg_set_rcq_size.3 rpma_conn_cfg_set_rq_size.3 rpma_conn_cfg_set_sq_size.3 rpma_conn_cfg_set_srq.3 rpma_conn_cfg_set_timeout.3 rpma_conn_delete.3 rpma_conn_disconnect.3 rpma_conn_get_compl_fd.3 rpma_conn_get_cq.3 rpma_conn_get_event_fd.3 rpma_conn_get_private_data.3 rpma_conn_get_qp_num.3 rpma_conn_get_rcq.3 rpma_conn_next_event.3 rpma_conn_req_connect.3 rpma_conn_req_delete.3 rpma_conn_req_get_private_data.3 rpma_conn_req_new.3 rpma_conn_req_recv.3 rpma_conn_wait.3 rpma_cq_get_fd.3 rpma_cq_get_wc.3 rpma_cq_wait.3 rpma_ep_get_fd.3 rpma_ep_listen.3 rpma_ep_next_conn_req.3 rpma_ep_shutdown.3 rpma_err_2str.3 rpma_flush.3 rpma_log_get_threshold.3 rpma_log_set_function.3 rpma_log_set_threshold.3 rpma_mr_advise.3 rpma_mr_dereg.3 rpma_mr_get_descriptor.3 rpma_mr_get_descriptor_size.3 rpma_mr_get_ptr.3 rpma_mr_get_size.3 rpma_mr_reg.3 rpma_mr_remote_delete.3 rpma_mr_remote_from_descriptor.3 rpma_mr_remote_get_flush_type.3 rpma_mr_remote_get_size.3 rpma_peer_cfg_delete.3 rpma_peer_cfg_from_descriptor.3 rpma_peer_cfg_get_descriptor.3 rpma_peer_cfg_get_descriptor_size.3 rpma_peer_cfg_get_direct_write_to_pmem.3 rpma_peer_cfg_new.3 rpma_peer_cfg_set_direct_write_to_pmem.3 rpma_peer_delete.3 rpma_peer_new.3 rpma_read.3 rpma_recv.3 rpma_send.3 rpma_send_with_imm."><meta property="og:type" content="article"><meta property="og:url" content="https://pmem.io/rpma/manpages/main/librpma.7/"><meta property="article:section" content="rpma"><meta charset=utf-8><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><title>librpma | PMDK</title><meta name=author content="PMem.io"><meta name=description content="Persistent Memory Development Kit (PMDK) provides support for transactional and atomic operations to keep the data consistent and durable.  PMDK is a collection of open-source libraries and tools that are available for both Linux and Windows OS.  PMDK facilitates persistent memory programming adoption with higher level language support.  Currently, Java, Python, Rust, Go, C and C++ support is fully validated and delivered on Linux and Windows.  This new generation of persistent memory from Intel has introduced a third memory tier (memory persistence, memory tiering).  In addition to memory and storage tiers, the persistent memory tier offers greater capacity than DRAM and significantly faster performance than storage.  Applications can access persistent memory-resident data structures in-place, like they do with traditional memory, eliminating the need to page blocks of data back and forth between memory and storage. PMDK provides a toolkit for memory hierarchy, memory caching, virtual memory and memory tiering.  PMDK-PMEM toolkit provides operational modes in either app direct mode or memory mode. App Direct Mode provides memory persistent, high availability less downtime and significantly faster storage.  In memory mode provides high memory capacity at lower cost and is transparent to applications.  Memory is volatile in memory mode and persistent in App Direct mode"><meta name=robots content="index, follow, archive"><link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700,900&display=swap" rel=stylesheet type=text/css><link rel=stylesheet href=/css/bootstrap.css type=text/css><link rel=stylesheet href=/css/style.css type=text/css><link rel=stylesheet href=/css/dark.css type=text/css><link rel=stylesheet href=/css/font-icons.css type=text/css><link rel=stylesheet href=/css/animate.css type=text/css><link rel=stylesheet href=/css/magnific-popup.css type=text/css><link rel=stylesheet href=/css/et-line.css type=text/css><link rel=stylesheet href=/css/components/bs-switches.css type=text/css><link rel=stylesheet href=/css/custom.css type=text/css><meta name=viewport content="initial-scale=1,viewport-fit=cover"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css type=text/css><link rel=stylesheet href="/css/colors.php?color=FE9603" type=text/css><link rel=stylesheet href=/css/template/fonts.css type=text/css><link rel=stylesheet href=/css/template/seo.css type=text/css></head><body class=stretched><div id=wrapper class=clearfix><header id=header class="transparent-header floating-header header-size-md sticky-header"><div id=header-wrap class=dark-mode><div class="container dark-mode"><div class=header-row><div id=logo class=logo_dark><a href=/ class=standard-logo data-dark-logo=images/logo-dark.png><img src=https://pmem.io/images/pmem_logo.png alt=PMem.io></a>
<a href=/ class=retina-logo data-dark-logo=images/logo-dark@2x.png><img src=https://pmem.io/images/pmem_logo.png alt=PMem.io></a></div><div id=logo class=logo_light><a href=/ class=standard-logo data-dark-logo=images/logo-dark.png><img src=https://pmem.io/images/pmem_logo_white.png alt=PMem.io></a>
<a href=/ class=retina-logo data-dark-logo=images/logo-dark@2x.png><img src=https://pmem.io/images/pmem_logo_white.png alt=PMem.io></a></div><div class=header-misc><div id=top-search class=header-misc-icon><a href=# id=top-search-trigger><i class=icon-line-search></i><i class=icon-line-cross></i></a></div><div class=top-links><ul class=top-links-container><li><div id=darkSwitch class="dark-mode header-misc-icon d-md-block"><a href=#><i id=darkSwitchToggle></i></a></div></li></ul></div></div><div id=primary-menu-trigger><svg class="svg-trigger" viewBox="0 0 100 100"><path d="m30 33h40c3.722839.0 7.5 3.126468 7.5 8.578427C77.5 47.030386 74.772971 50 70 50H50"/><path d="m30 50h40"/><path d="m70 67H30s-7.5-.802118-7.5-8.365747C22.5 51.070624 30 50 30 50h20"/></svg></div><nav class="primary-menu with-arrows"><ul class=menu-container><li class="menu-item mega-menu"><div class=menu-link><div><a href=/developer-hub>Developer Hub</a></div></div><div class="mega-menu-content mega-menu-style-2 px-0"><div class="container dark-mode"><div class=row><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class=fbox-content><p class=fw-bold>For Developers</p><p>Everything you need to know about Persistent Memory.</p></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/persistent-memory/getting-started-guide><p>Get started <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/pmdk><p>PMDK <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/repoindex><p>PMem Repositories <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/pmemkv><p>PMemKV <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/pmemstream><p>PMemStream <i class=icon-angle-right></i></p></a></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/memkind><p>Memkind <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/miniasync><p>MiniAsync <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/community/#newsletter><p>Newsletter <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://tieredmemdb.github.io/><p>TieredMemDB <i class=icon-angle-right></i></p></a></div></div></div></div></div></div></li><li class="menu-item mega-menu"><div class=menu-link><div><a href=/learn>Learn</a></div></div><div class="mega-menu-content mega-menu-style-2 px-0"><div class="container dark-mode"><div class=row><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class=fbox-content><p class=fw-bold>Access our Documentation</p><p>Learn more about Persistent Memory features and capabilities.</p></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/books><p>Books <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/persistent-memory/><p>Docs <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/glossary><p>Glossary <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/ipmctl-user-guide/><p>ipmctl User Guide <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/ndctl-user-guide/><p>ndctl User Guide <i class=icon-angle-right></i></p></a></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/faq><p>FAQ <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/knowledgebase><p>Knowledge base <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/tutorials><p>Tutorials <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/videos><p>Videos <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/webinars><p>Webinars <i class=icon-angle-right></i></p></a></div></div></div></div></div></div></li><li class="menu-item mega-menu"><div class=menu-link><div><a href=/community>Community</a></div></div><div class="mega-menu-content mega-menu-style-2 px-0"><div class="container dark-mode"><div class=row><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class=fbox-content><p class=fw-bold>Get Connected</p><p>Join an ever-growing community of PMDK-PMEM developers online or in person.</p></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/events><p>Events <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://groups.google.com/group/pmem><p>Forum <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://pmem-io.slack.com/join/shared_invite/enQtNzU4MzQ2Mzk3MDQwLWQ1YThmODVmMGFkZWI0YTdhODg4ODVhODdhYjg3NmE4N2ViZGI5NTRmZTBiNDYyOGJjYTIyNmZjYzQxODcwNDg#/shared-invite/email><p>Slack channel <i class=icon-angle-right></i></p></a></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/announcements><p>Announcements <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/blog/2021/10/how-to-contribute-to-pmem.io/><p>Contribute <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/community/#newsletter><p>Newsletter <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/community/#social-media><p>Social Media <i class=icon-angle-right></i></p></a></div></div></div></div></div></div></li><li class=menu-item><a class=menu-link href=https://pmem.io/solutions><div>Solutions</div></a></li><li class=menu-item><a class=menu-link href=https://pmem.io/blog><div>Blog</div></a></li><li class=menu-item><a class=menu-link href=https://pmem.io/about><div>About</div></a></li></ul></nav><form class=top-search-form method=get><input id=bcs-searchbox aria-label="Search input" type=text name=q class="form-control bcs-searchbox pt-4" placeholder="Type & Hit Enter.." autocomplete=off></form></div></div></div><div class="header-wrap-clone dark-mode"></div></header><div id=customSearch><div id=bcs_js_snippet></div></div><section id=page-title class="page-title-parallax page-title-center page-title-dark include-header skrollable skrollable-between" style="background-image:url('');background-size:cover;padding:120px 0;margin-top:-157.05px"><div class="container clearfix mt-4"><div class="slider-title text-light"><h2 style=color:#fff>librpma API version 1.1.0</h2></div></div></section><div class="section m-0 bg-transparent library-section dark-mode"><div class=container><div class="row justify-content-between"><div class="col mt-0 lib-content"><a href=../librpma.7.html>librpma.7</a>
<a href=../rpma_atomic_write.3.html>rpma_atomic_write.3</a>
<a href=../rpma_conn_apply_remote_peer_cfg.3.html>rpma_conn_apply_remote_peer_cfg.3</a>
<a href=../rpma_conn_cfg_delete.3.html>rpma_conn_cfg_delete.3</a>
<a href=../rpma_conn_cfg_get_compl_channel.3.html>rpma_conn_cfg_get_compl_channel.3</a>
<a href=../rpma_conn_cfg_get_cq_size.3.html>rpma_conn_cfg_get_cq_size.3</a>
<a href=../rpma_conn_cfg_get_rcq_size.3.html>rpma_conn_cfg_get_rcq_size.3</a>
<a href=../rpma_conn_cfg_get_rq_size.3.html>rpma_conn_cfg_get_rq_size.3</a>
<a href=../rpma_conn_cfg_get_sq_size.3.html>rpma_conn_cfg_get_sq_size.3</a>
<a href=../rpma_conn_cfg_get_srq.3.html>rpma_conn_cfg_get_srq.3</a>
<a href=../rpma_conn_cfg_get_timeout.3.html>rpma_conn_cfg_get_timeout.3</a>
<a href=../rpma_conn_cfg_new.3.html>rpma_conn_cfg_new.3</a>
<a href=../rpma_conn_cfg_set_compl_channel.3.html>rpma_conn_cfg_set_compl_channel.3</a>
<a href=../rpma_conn_cfg_set_cq_size.3.html>rpma_conn_cfg_set_cq_size.3</a>
<a href=../rpma_conn_cfg_set_rcq_size.3.html>rpma_conn_cfg_set_rcq_size.3</a>
<a href=../rpma_conn_cfg_set_rq_size.3.html>rpma_conn_cfg_set_rq_size.3</a>
<a href=../rpma_conn_cfg_set_sq_size.3.html>rpma_conn_cfg_set_sq_size.3</a>
<a href=../rpma_conn_cfg_set_srq.3.html>rpma_conn_cfg_set_srq.3</a>
<a href=../rpma_conn_cfg_set_timeout.3.html>rpma_conn_cfg_set_timeout.3</a>
<a href=../rpma_conn_delete.3.html>rpma_conn_delete.3</a>
<a href=../rpma_conn_disconnect.3.html>rpma_conn_disconnect.3</a>
<a href=../rpma_conn_get_compl_fd.3.html>rpma_conn_get_compl_fd.3</a>
<a href=../rpma_conn_get_cq.3.html>rpma_conn_get_cq.3</a>
<a href=../rpma_conn_get_event_fd.3.html>rpma_conn_get_event_fd.3</a>
<a href=../rpma_conn_get_private_data.3.html>rpma_conn_get_private_data.3</a>
<a href=../rpma_conn_get_qp_num.3.html>rpma_conn_get_qp_num.3</a>
<a href=../rpma_conn_get_rcq.3.html>rpma_conn_get_rcq.3</a>
<a href=../rpma_conn_next_event.3.html>rpma_conn_next_event.3</a>
<a href=../rpma_conn_req_connect.3.html>rpma_conn_req_connect.3</a>
<a href=../rpma_conn_req_delete.3.html>rpma_conn_req_delete.3</a>
<a href=../rpma_conn_req_get_private_data.3.html>rpma_conn_req_get_private_data.3</a>
<a href=../rpma_conn_req_new.3.html>rpma_conn_req_new.3</a>
<a href=../rpma_conn_req_recv.3.html>rpma_conn_req_recv.3</a>
<a href=../rpma_conn_wait.3.html>rpma_conn_wait.3</a>
<a href=../rpma_cq_get_fd.3.html>rpma_cq_get_fd.3</a>
<a href=../rpma_cq_get_wc.3.html>rpma_cq_get_wc.3</a>
<a href=../rpma_cq_wait.3.html>rpma_cq_wait.3</a>
<a href=../rpma_ep_get_fd.3.html>rpma_ep_get_fd.3</a>
<a href=../rpma_ep_listen.3.html>rpma_ep_listen.3</a>
<a href=../rpma_ep_next_conn_req.3.html>rpma_ep_next_conn_req.3</a>
<a href=../rpma_ep_shutdown.3.html>rpma_ep_shutdown.3</a>
<a href=../rpma_err_2str.3.html>rpma_err_2str.3</a>
<a href=../rpma_flush.3.html>rpma_flush.3</a>
<a href=../rpma_log_get_threshold.3.html>rpma_log_get_threshold.3</a>
<a href=../rpma_log_set_function.3.html>rpma_log_set_function.3</a>
<a href=../rpma_log_set_threshold.3.html>rpma_log_set_threshold.3</a>
<a href=../rpma_mr_advise.3.html>rpma_mr_advise.3</a>
<a href=../rpma_mr_dereg.3.html>rpma_mr_dereg.3</a>
<a href=../rpma_mr_get_descriptor.3.html>rpma_mr_get_descriptor.3</a>
<a href=../rpma_mr_get_descriptor_size.3.html>rpma_mr_get_descriptor_size.3</a>
<a href=../rpma_mr_get_ptr.3.html>rpma_mr_get_ptr.3</a>
<a href=../rpma_mr_get_size.3.html>rpma_mr_get_size.3</a>
<a href=../rpma_mr_reg.3.html>rpma_mr_reg.3</a>
<a href=../rpma_mr_remote_delete.3.html>rpma_mr_remote_delete.3</a>
<a href=../rpma_mr_remote_from_descriptor.3.html>rpma_mr_remote_from_descriptor.3</a>
<a href=../rpma_mr_remote_get_flush_type.3.html>rpma_mr_remote_get_flush_type.3</a>
<a href=../rpma_mr_remote_get_size.3.html>rpma_mr_remote_get_size.3</a>
<a href=../rpma_peer_cfg_delete.3.html>rpma_peer_cfg_delete.3</a>
<a href=../rpma_peer_cfg_from_descriptor.3.html>rpma_peer_cfg_from_descriptor.3</a>
<a href=../rpma_peer_cfg_get_descriptor.3.html>rpma_peer_cfg_get_descriptor.3</a>
<a href=../rpma_peer_cfg_get_descriptor_size.3.html>rpma_peer_cfg_get_descriptor_size.3</a>
<a href=../rpma_peer_cfg_get_direct_write_to_pmem.3.html>rpma_peer_cfg_get_direct_write_to_pmem.3</a>
<a href=../rpma_peer_cfg_new.3.html>rpma_peer_cfg_new.3</a>
<a href=../rpma_peer_cfg_set_direct_write_to_pmem.3.html>rpma_peer_cfg_set_direct_write_to_pmem.3</a>
<a href=../rpma_peer_delete.3.html>rpma_peer_delete.3</a>
<a href=../rpma_peer_new.3.html>rpma_peer_new.3</a>
<a href=../rpma_read.3.html>rpma_read.3</a>
<a href=../rpma_recv.3.html>rpma_recv.3</a>
<a href=../rpma_send.3.html>rpma_send.3</a>
<a href=../rpma_send_with_imm.3.html>rpma_send_with_imm.3</a>
<a href=../rpma_srq_cfg_delete.3.html>rpma_srq_cfg_delete.3</a>
<a href=../rpma_srq_cfg_get_rcq_size.3.html>rpma_srq_cfg_get_rcq_size.3</a>
<a href=../rpma_srq_cfg_get_rq_size.3.html>rpma_srq_cfg_get_rq_size.3</a>
<a href=../rpma_srq_cfg_new.3.html>rpma_srq_cfg_new.3</a>
<a href=../rpma_srq_cfg_set_rcq_size.3.html>rpma_srq_cfg_set_rcq_size.3</a>
<a href=../rpma_srq_cfg_set_rq_size.3.html>rpma_srq_cfg_set_rq_size.3</a>
<a href=../rpma_srq_delete.3.html>rpma_srq_delete.3</a>
<a href=../rpma_srq_get_rcq.3.html>rpma_srq_get_rcq.3</a>
<a href=../rpma_srq_new.3.html>rpma_srq_new.3</a>
<a href=../rpma_srq_recv.3.html>rpma_srq_recv.3</a>
<a href=../rpma_utils_conn_event_2str.3.html>rpma_utils_conn_event_2str.3</a>
<a href=../rpma_utils_get_ibv_context.3.html>rpma_utils_get_ibv_context.3</a>
<a href=../rpma_utils_ibv_context_is_odp_capable.3.html>rpma_utils_ibv_context_is_odp_capable.3</a>
<a href=../rpma_write.3.html>rpma_write.3</a>
<a href=../rpma_write_with_imm.3.html>rpma_write_with_imm.3</a></br></br><h1 id=name>NAME</h1><p><strong>librpma</strong> - remote persistent memory access library</p><h1 id=synopsis>SYNOPSIS</h1><pre><code>      #include &lt;librpma.h&gt;
      cc ... -lrpma
</code></pre><h1 id=description>DESCRIPTION</h1><p>librpma is a C library to simplify accessing persistent memory (PMem) on
remote hosts over Remote Direct Memory Access (RDMA).</p><p>The librpma library provides two possible schemes of operation: Remote
Memory Access and Messaging. Both of them are available over a
connection established between two peers. Both of these schemes can make
use of PMem as well as DRAM for the sake of building efficient and
scalable Remote Persistent Memory Accessing (RPMA) applications.</p><h1 id=remote-memory-access>REMOTE MEMORY ACCESS</h1><p>The librpma library implements four basic API calls dedicated for
accessing a remote memory:</p><ul><li><p><strong>rpma_read</strong>() - initiates transferring data from the remote
memory to the local memory,</p></li><li><p><strong>rpma_write</strong>() - initiates transferring data from the local
memory to the remote memory),</p></li><li><p><strong>rpma_atomic_write</strong>() - works like <strong>rpma_write</strong>(), but it
allows transferring 8 bytes of data (RPMA_ATOMIC_WRITE_ALIGNMENT)
and storing them atomically in the remote memory (see
<strong>rpma_atomic_write</strong>(3) for details and restrictions), and:</p></li><li><p><strong>rpma_flush</strong>() - initiates finalizing a transfer of data to the
remote memory. Possible types of <strong>rpma_flush</strong>() operation:</p><ul><li><p>RPMA_FLUSH_TYPE_PERSISTENT - flush data down to the
persistent domain,</p></li><li><p>RPMA_FLUSH_TYPE_VISIBILITY - flush data deep enough to make
it visible on the remote node.</p></li></ul></li></ul><p>All the above functions use the attribute flags to set the completion
notification indicator:</p><ul><li><p>RPMA_F_COMPLETION_ON_ERROR - generates the completion only on
error</p></li><li><p>RPMA_F_COMPLETION_ALWAYS - generates the completion regardless of
a result of the operation.</p></li></ul><p>All of these operations are considered as finished when the respective
completion is generated.</p><h1 id=direct-write-to-pmem>DIRECT WRITE TO PMEM</h1><p><strong>Direct Write to PMem</strong> is a feature of a platform and its
configuration which allows an RDMA-capable network interface to write
data to platform's PMem in a persistent way. It may be impossible
because of e.g. caching mechanisms existing on the data's way. When
<strong>Direct Write to PMem</strong> is impossible, operating in the way assuming it
is possible may corrupt data on PMem, so this is why <strong>Direct Write to
PMem</strong> is not enabled by default.</p><p>On the current Intel platforms, the only thing you have to do in order
to enable <strong>Direct Write to PMem</strong> is turning off Intel Direct Data I/O
(DDIO). Sometimes, you can turn off DDIO either globally for the whole
platform or for a specific PCIe Root Port. For details, please see the
manual of your platform.</p><p>When you have a platform which allows <strong>Direct Write to PMem</strong>, you have
to declare this is the case in your peer's configuration. The peer's
configuration has to be transferred to all the peers which want to
execute <strong>rpma_flush</strong>() with RPMA_FLUSH_TYPE_PERSISTENT against the
platform's PMem and applied to the connection object which safeguards
access to PMem.</p><ul><li><p><strong>rpma_peer_cfg_set_direct_write_to_pmem</strong>() - declare
<strong>Direct Write to PMem</strong> support</p></li><li><p><strong>rpma_peer_cfg_get_descriptor</strong>() - get the descriptor of the
peer configuration</p></li><li><p><strong>rpma_peer_cfg_from_descriptor</strong>() - create a peer
configuration from the descriptor</p></li><li><p><strong>rpma_conn_apply_remote_peer_cfg</strong>() - apply remote peer cfg
to the connection</p></li></ul><p>For details on how to use these APIs please see
<a href=https://github.com/pmem/rpma/tree/main/examples/05-flush-to-persistent>https://github.com/pmem/rpma/tree/main/examples/05-flush-to-persistent</a>.</p><h1 id=client-operation>CLIENT OPERATION</h1><p>A client is the active side of the process of establishing a connection.
A role of the peer during the process of establishing connection does
not determine direction of the data flow (neither via Remote Memory
Access nor via Messaging). After establishing the connection both peers
have the same capabilities.</p><p>The client, in order to establish a connection, has to perform the
following steps:</p><ul><li><p><strong>rpma_conn_req_new</strong>() - create a new outgoing connection
request object</p></li><li><p><strong>rpma_conn_req_connect</strong>() - initiate processing the connection
request</p></li><li><p><strong>rpma_conn_next_event</strong>() - wait for the RPMA_CONN_ESTABLISHED
event</p></li></ul><p>After establishing the connection both peers can perform Remote Memory
Access and/or Messaging over the connection.</p><p>The client, in order to close a connection, has to perform the following
steps:</p><ul><li><p><strong>rpma_conn_disconnect</strong>() - initiate disconnection</p></li><li><p><strong>rpma_conn_next_event</strong>() - wait for the RPMA_CONN_CLOSED
event</p></li><li><p><strong>rpma_conn_delete</strong>() - delete the closed connection</p></li></ul><h1 id=server-operation>SERVER OPERATION</h1><p>A server is the passive side of the process of establishing a
connection. Note that after establishing the connection both peers have
the same capabilities.</p><p>The server, in order to establish a connection, has to perform the
following steps:</p><ul><li><p><strong>rpma_ep_listen</strong>() - create a listening endpoint</p></li><li><p><strong>rpma_ep_next_conn_req</strong>() - obtain an incoming connection
request</p></li><li><p><strong>rpma_conn_req_connect</strong>() - initiate connecting the connection
request</p></li><li><p><strong>rpma_conn_next_event</strong>() - wait for the RPMA_CONN_ESTABLISHED
event</p></li></ul><p>After establishing the connection both peers can perform Remote Memory
Access and/or Messaging over the connection.</p><p>The server, in order to close a connection, has to perform the following
steps:</p><ul><li><p><strong>rpma_conn_next_event</strong>() - wait for the RPMA_CONN_CLOSED
event</p></li><li><p><strong>rpma_conn_disconnect</strong>() - disconnect the connection</p></li><li><p><strong>rpma_conn_delete</strong>() - delete the closed connection</p></li></ul><p>When no more incoming connections are expected, the server can stop
waiting for them:</p><ul><li><strong>rpma_ep_shutdown</strong>() - stop listening and delete the endpoint</li></ul><h1 id=memory-management>MEMORY MANAGEMENT</h1><p>Every piece of memory (either volatile or persistent) must be registered
and its usage must be specified in order to be used in Remote Memory
Access or Messaging. This can be done using the following memory
management librpma functions:</p><ul><li><p><strong>rpma_mr_reg</strong>() which registers a memory region and creates a
local memory registration object and</p></li><li><p><strong>rpma_mr_dereg</strong>() which deregisters the memory region and
deletes the local memory registration object.</p></li></ul><p>A description of the registered memory region sometimes has to be
transferred via network to the other side of the connection. In order to
do that a network-transferable description of the provided memory region
(called 'descriptor') has to be created using
<strong>rpma_mr_get_descriptor</strong>(). On the other side of the connection the
received descriptor should be decoded using
<strong>rpma_mr_remote_from_descriptor</strong>(). It creates a remote memory
region's structure that allows for Remote Memory Access.</p><h1 id=messaging>MESSAGING</h1><p>The librpma messaging API allows transferring messages (buffers of
arbitrary data) between the peers. Transferring messages requires
preparing buffers (memory regions) on the remote side to receive the
sent data. The received data are written to those dedicated buffers and
the sender does not have to have a respective remote memory region
object to send a message. The memory buffers used for messaging have to
be registered using <strong>rpma_mr_reg</strong>() prior to <strong>rpma_send</strong>() or
<strong>rpma_recv</strong>() function call.</p><p>The librpma library implements the following messaging API:</p><ul><li><p><strong>rpma_send</strong>() - initiates the send operation which transfers a
message from the local memory to other side of the connection,</p></li><li><p><strong>rpma_recv</strong>() - initiates the receive operation which prepares a
buffer for a message sent from other side of the connection,</p></li><li><p><strong>rpma_conn_req_recv</strong>() works as <strong>rpma_recv</strong>(), but it may be
used before the connection is established.</p></li></ul><p>All of these operations are considered as finished when the respective
completion is generated.</p><h1 id=completions>COMPLETIONS</h1><p>RDMA operations generate complitions that notify a user that the
respective operation has been completed.</p><p>The following operations are available in librpma:</p><ul><li><p>IBV_WC_RDMA_READ - RMA read operation</p></li><li><p>IBV_WC_RDMA_WRITE - RMA write operation</p></li><li><p>IBV_WC_SEND - messaging send operation</p></li><li><p>IBV_WC_RECV - messaging receive operation</p></li><li><p>IBV_WC_RECV_RDMA_WITH_IMM - messaging receive operation for RMA
write operation with immediate data</p></li></ul><p>All operations generate completion on error. The operations posted with
the <strong>RPMA_F_COMPLETION_ALWAYS</strong> flag also generate a completion on
success. Completion codes are reused from the libibverbs library, where
the IBV_WC_SUCCESS status indicates the successful completion of an
operation. Completions are collected in the completion queue (CQ) (see
the <strong>QUEUES, PERFORMANCE AND RESOURCE USE</strong> section for more details on
queues).</p><p>The librpma library implements the following API for handling
completions:</p><ul><li><p><strong>rpma_conn_get_cq</strong>() gets the connection's main CQ,</p></li><li><p><strong>rpma_conn_get_rcq</strong>() gets the connection's receive CQ,</p></li><li><p><strong>rpma_cq_wait</strong>() waits for an incoming completion from the
specified CQ (main or receive CQ) - if it succeeds the completion
can be collected using <strong>rpma_cq_get_wc</strong>(),</p></li><li><p><strong>rpma_cq_get_wc</strong>() receives the next available completion of an
already posted operation.</p></li></ul><h1 id=peer>PEER</h1><p>A peer is an abstraction representing an RDMA-capable device. All other
RPMA objects have to be created in the context of a peer. A peer allows
one to:</p><ul><li><p>establish connections (Client Operation)</p></li><li><p>register memory regions (Memory Management)</p></li><li><p>create endpoints for listening for incoming connections (Server
Operation)</p></li></ul><p>At the beginning, in order to create a peer, a user has to obtain an
RDMA device context by the given IPv4/IPv6 address using
<strong>rpma_utils_get_ibv_context</strong>(). Then a new peer object can be
created using <strong>rpma_peer_new</strong>() and deleted using
<strong>rpma_peer_delete</strong>().</p><h1 id=synchronous-and-asynchronous-modes>SYNCHRONOUS AND ASYNCHRONOUS MODES</h1><p>By default, all endpoints and connections operate in the synchronous
mode where:</p><ul><li><p><strong>rpma_ep_next_conn_req</strong>(),</p></li><li><p><strong>rpma_cq_wait</strong>() and</p></li><li><p><strong>rpma_conn_get_next_event</strong>()</p></li></ul><p>are blocking calls. You can make those API calls non-blocking by
modifying the respective file descriptors:</p><ul><li><p><strong>rpma_ep_get_fd</strong>() - provides a file descriptor for
<strong>rpma_ep_next_conn_req</strong>()</p></li><li><p><strong>rpma_cq_get_fd</strong>() - provides a file descriptor for
<strong>rpma_cq_wait</strong>()</p></li><li><p><strong>rpma_conn_get_event_fd</strong>() - provides a file descriptor for
<strong>rpma_conn_get_next_event</strong>()</p></li></ul><p>When you have a file descriptor, you can make it non-blocking using
<strong>fcntl</strong>(2) as follows:</p><pre><code>        int ret = fcntl(fd, F_GETFL);
        fcntl(fd, F_SETFL, flags | O_NONBLOCK);
</code></pre><p>Such change makes the respective API call non-blocking automatically.</p><p>The provided file descriptors can also be used for scalable I/O handling
like <strong>epoll</strong>(7).</p><p>Please see the example showing how to make use of RPMA file descriptors:
<a href=https://github.com/pmem/rpma/tree/main/examples/06-multiple-connections>https://github.com/pmem/rpma/tree/main/examples/06-multiple-connections</a></p><h1 id=queues-performance-and-resource-use>QUEUES, PERFORMANCE AND RESOURCE USE</h1><p><strong>Remote Memory Access</strong> operations, <strong>Messaging</strong> operations and their
<strong>Completions</strong> consume space in queues allocated in an RDMA-capable
network interface (RNIC) hardware for each of the connections. You must
be aware of the existence of these queues:</p><ul><li><p>completion queue <strong>(CQ)</strong> where completions of operations are
placed, either when a completion was required by a user
(RPMA_F_COMPLETION_ALWAYS) or a completion with an error
occurred. All <strong>Remote Memory Access</strong> operations and <strong>Messaging</strong>
operations can consume <strong>CQ</strong> space.</p></li><li><p>send queue <strong>(SQ)</strong> where all <strong>Remote Memory Access</strong> operations
and <strong>rpma_send</strong>() operations are placed before they are executed
by RNIC.</p></li><li><p>receive queue <strong>(RQ)</strong> where <strong>rpma_recv</strong>() entries are placed
before they are consumed by the <strong>rpma_send</strong>() coming from another
side of the connection.</p></li></ul><p>You must assume <strong>SQ</strong> and <strong>RQ</strong> entries occupy the place in their
respective queue till:</p><ul><li><p>a respective operation's completion is generated or</p></li><li><p>a completion of an operation, which was scheduled later, is
generated.</p></li></ul><p>You must also be aware that RNIC has limited resources so it is
impossible to store a very long set of queues for many possibly existing
connections. If all of the queues will not fit into RNIC's resources it
will start using the platform's memory for this purpose. In this case,
the performance will be degraded because of inevitable cache misses.</p><p>Because the length of queues has so profound impact on the performance
of RPMA application you can configure the length of each of the queues
separately for each of the connections:</p><ul><li><p><strong>rpma_conn_cfg_set_cq_size</strong>() - set length of <strong>CQ</strong></p></li><li><p><strong>rpma_conn_cfg_set_sq_size</strong>() - set length of <strong>SQ</strong></p></li><li><p><strong>rpma_conn_cfg_set_rq_size</strong>() - set length of <strong>RQ</strong></p></li></ul><p>When the connection configuration object is ready it has to be used for
either <strong>rpma_conn_req_new</strong>() or <strong>rpma_ep_next_conn_req</strong>() for
the settings to take effect.</p><h1 id=thread-safety>THREAD SAFETY</h1><p>The analysis of thread safety of the librpma library is described in
details in the THREAD_SAFETY.md file:</p><pre><code>        https://github.com/pmem/rpma/blob/main/THREAD_SAFETY.md
</code></pre><h1 id=on-demand-paging-support>ON-DEMAND PAGING SUPPORT</h1><p>On-Demand-Paging (ODP) is a technique that simplifies the memory
registration process (for example, applications no longer need to pin
down the underlying physical pages of the address space and track the
validity of the mappings). On-Demand Paging is available if both the
hardware and the kernel support it. The detailed description of ODP can
be found here:</p><pre><code>     https://community.mellanox.com/s/article/understanding-on-demand-paging--odp-x
</code></pre><p>State of ODP support can be checked using the
<strong>rpma_utils_ibv_context_is_odp_capable</strong>() function that queries
the RDMA device context's capabilities and checks if it supports
On-Demand Paging.</p><p>The librpma library uses ODP automatically if it is supported. ODP
support is required to register PMem memory region mapped from File
System DAX (FSDAX).</p><h1 id=debugging-and-error-handling>DEBUGGING AND ERROR HANDLING</h1><p>If a librpma function may fail, it returns a negative error code.
Checking if the returned value is non-negative is the only
programmatically available way to verify if the API call succeeded. The
exact meaning of all error codes is described in the manual of each
function.</p><p>The librpma library implements the logging API which may give additional
information in case of an error and during normal operation as well,
according to the current logging threshold levels.</p><p>The function that will handle all generated log messages can be set
using <strong>rpma_log_set_function</strong>(). The logging function can be either
the default logging function (built into the library) or a user-defined,
thread-safe, function. The default logging function can write messages
to <strong>syslog</strong>(3) and <strong>stderr</strong>(3). The logging threshold level can be
set or got using <strong>rpma_log_set_threshold</strong>() or
<strong>rpma_log_get_threshold</strong>() respectively.</p><p>There is an example of the usage of the logging functions:
<a href=https://github.com/pmem/rpma/tree/main/examples/log>https://github.com/pmem/rpma/tree/main/examples/log</a></p><h1 id=examples>EXAMPLES</h1><p>See <a href=https://github.com/pmem/rpma/tree/main/examples>https://github.com/pmem/rpma/tree/main/examples</a> for examples of
using the librpma API.</p><h1 id=acknowledgements>ACKNOWLEDGEMENTS</h1><p>librpma is built on the top of libibverbs and librdmacm APIs.</p><h1 id=deprecating>DEPRECATING</h1><p>Using of the API calls which are marked as deprecated should be avoided,
because they will be removed in a new major release.</p><p>NOTE: API calls deprecated in 0.X release will be removed in <strong>0.</strong>(X+1)
release usually.</p><h1 id=see-also>SEE ALSO</h1><p><a href=https://pmem.io/rpma/>https://pmem.io/rpma/</a></p></div></div><div class=divider></div><p class=text-center><small>The contents of this web site and the associated <a href=https://github.com/pmem>GitHub repositories</a> are BSD-licensed open source.</small></p></div></div><footer id=footer class="border-0 bg-white"><div id=copyrights><div class="container clearfix"><div class="row justify-content-between col-mb-30"><div class="col-12 col-lg-auto text-center text-lg-start"><div id=logo><a href=/ class=standard-logo data-dark-logo=images/logo-dark.png><img src=https://pmem.io/images/pmem_logo.png alt="PMem Logo"></a>
<a href=/ class=retina-logo data-dark-logo=images/logo-dark@2x.png><img src=https://pmem.io/images/pmem_logo.png alt="PMem Logo"></a></div></div><div class="col-12 col-lg-auto text-center text-lg-end"><div class="copyrights-menu copyright-links clearfix text-uppercase"><a href=https://pmem.io/about>about</a>/
<a href=https://pmem.io/blog>blog</a>/
<a href=https://pmem.io/community>community</a>/
<a href=https://pmem.io/cookies.html>Cookies</a>/
<a href=https://pmem.io/developer-hub>developer Hub</a>/
<a href=https://pmem.io/learn>learn</a>/
<a href=https://pmem.io/privacy.html>Privacy</a>/
<a href=https://pmem.io/solutions>solutions</a>/
<a href=https://pmem.io/terms.html>Terms</a></div><div class="col-lg-auto text-center mt-0"><p>Copyright &copy; 2022 pmem.io</p></div></div></div></div></div></footer></div><div id=gotoTop class=icon-angle-up></div><script src=/js/jquery.js></script>
<script src=/js/plugins.min.js></script>
<script src=/js/custom.js></script>
<script src=/js/darkmode.js></script>
<script src=/js/functions.js></script>
<script type=text/javascript src="https://ui.customsearch.ai/api/ux/rendering-js?customConfig=011a90aa-26ea-46b5-bf60-4b5b407c72c6&market=en-US&version=latest&q="></script></body></html>